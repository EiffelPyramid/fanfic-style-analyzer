#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import streamlit as st
import pandas as pd
import jieba
import jieba.posseg as pseg
from gensim.models import FastText
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import re
import os
import streamlit.components.v1 as components
from lime.lime_text import LimeTextExplainer

# ==========================================
# 0. é¡µé¢é…ç½®ä¸å­—ä½“å¤„ç† (å®‰å…¨ç‰ˆ)
# ==========================================
st.set_page_config(page_title="æ–‡é£æŒ‡çº¹åˆ†æå®éªŒå®¤ (Pro)", layout="wide")

@st.cache_resource
def get_font_prop():
    font_path = "simhei.ttf"
    if not os.path.exists(font_path):
        return None
    try:
        if os.path.getsize(font_path) / (1024 * 1024) < 1: return None
        return fm.FontProperties(fname=font_path)
    except:
        return None

my_font_prop = get_font_prop()

# ==========================================
# 1. æ ¸å¿ƒå·¥å…·å‡½æ•°
# ==========================================

def read_content_safe(file_obj, limit=None):
    try:
        file_obj.seek(0)
        content_bytes = file_obj.read()
        text = content_bytes.decode('utf-8')
    except UnicodeDecodeError:
        try:
            text = content_bytes.decode('gbk')
        except UnicodeDecodeError:
            text = content_bytes.decode('utf-8', errors='ignore')
    if limit: return text[:limit]
    return text

def basic_clean(text):
    if not isinstance(text, str): return ""
    text = re.sub(r'ç¬¬.+?ç« .*', '', text)
    text = re.sub(r'Chapter.*', '', text)
    text = re.sub(r'\[\d+\]|[\u2460-\u24FF]', '', text)
    punctuation_map = {',': 'ï¼Œ', '!': 'ï¼', '?': 'ï¼Ÿ', '(': 'ï¼ˆ', ')': 'ï¼‰', ':': 'ï¼š', ';': 'ï¼›'}
    for eng_punc, chi_punc in punctuation_map.items():
        text = text.replace(eng_punc, chi_punc)
    return text

def smart_chunking(text, min_length=300):
    lines = text.split("\n")
    final_chunks = []
    current_chunk = ""
    for line in lines:
        line = line.strip()
        if len(line) < 2: continue
        current_chunk += line + " "
        if len(current_chunk) >= min_length:
            final_chunks.append(current_chunk)
            current_chunk = ""
    if len(current_chunk) > 50:
        final_chunks.append(current_chunk)
    return final_chunks

def get_style_tokens(text, blocklist):
    # æ ¸å¿ƒï¼šä¿ç•™ä¸åœ¨é»‘åå•é‡Œçš„è¯
    text = basic_clean(text)
    words = jieba.lcut(text)
    return [w for w in words if w not in blocklist and not w.isspace()]

def generate_blocklist_from_files(uploaded_files):
    sample_text = ""
    for uploaded_file in uploaded_files:
        content = read_content_safe(uploaded_file)
        sample_text += basic_clean(content)
    words = pseg.cut(sample_text)
    candidates = []
    target_flags = {'nr', 'ns', 'nz', 'nt', 'per', 'loc'}
    for w, f in words:
        if len(w) > 1 and f in target_flags:
            candidates.append(w)
    from collections import Counter
    blocklist = set([w for w, c in Counter(candidates).most_common(500)])
    return blocklist

# ==========================================
# 2. ç½‘ç«™ç•Œé¢ UI
# ==========================================

st.title("ğŸ•µï¸â€â™‚ï¸ æ–‡é£åˆ†æå®éªŒå®¤")
st.markdown("""
ä¸Šä¼ æŸä½ä½œå®¶çš„åŸè‘—ï¼Œå†è¾“å…¥ä½ çš„åŒäººæ–‡æœ¬ï¼Œç®—æ³•å°†é€šè¿‡è™šè¯ã€å¥å¼ç­‰åˆ¤æ–­åŒäººæ–‡æœ¬çš„è¿˜åŸåº¦ï¼Œå¹¶é«˜äº®æ˜¾ç¤ºæ–‡ä¸­å“ªäº›è¯å¥æœ€å…·æœ‰åŸè‘—ç¥éŸµã€‚
""")

col1, col2 = st.columns([1, 2])

with col1:
    st.header("Step 1: å»ºç«‹åŸºå‡†")
    st.info("è¯·ä¸Šä¼ åŸè‘— TXT æ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰")
    uploaded_originals = st.file_uploader("ä¸Šä¼ åŸè‘— (æ”¯æŒ .txt)", type="txt", accept_multiple_files=True)

    st.header("Step 2: è¾“å…¥æµ‹è¯•æ–‡æœ¬")
    fanfic_text = st.text_area("åœ¨æ­¤ç²˜è´´ä½ çš„åŒäººæ–‡æœ¬ï¼š", height=200, placeholder="å»ºè®®ç²˜è´´ 500 å­—ä»¥ä¸Šçš„æ®µè½...")

    start_btn = st.button("ğŸš€ å¼€å§‹æ–‡é£åˆ†æ", type="primary")

# ==========================================
# 3. ä¸»é€»è¾‘æ§åˆ¶å™¨
# ==========================================

if start_btn:
    if not uploaded_originals or not fanfic_text.strip():
        st.error("è¯·ç¡®ä¿å·²ä¸Šä¼ åŸè‘—å¹¶è¾“å…¥äº†æµ‹è¯•æ–‡æœ¬ã€‚")
    else:
        with col2:
            status = st.status("æ­£åœ¨å¯åŠ¨æ–‡é£è§£æå¼•æ“...", expanded=True)
            
            # --- A: é¢„å¤„ç† ---
            status.write("ğŸ“– ç”Ÿæˆå®ä½“åœç”¨è¯è¡¨...")
            blocklist = generate_blocklist_from_files(uploaded_originals)
            
            status.write("âœ‚ï¸ åˆ‡åˆ†ä¸æ¸…æ´—...")
            original_docs = []
            for u_file in uploaded_originals:
                content = read_content_safe(u_file)
                chunks = smart_chunking(content)
                for chunk in chunks:
                    tokens = get_style_tokens(chunk, blocklist)
                    if len(tokens) > 50: original_docs.append(tokens)
            
            # å¯¹åŒäººæ–‡æœ¬ï¼Œä¸ºäº†LIMEåˆ†æï¼Œæˆ‘ä»¬æœ€å¥½ä¸è¦åˆ‡å¾—å¤ªç¢ï¼Œå–å‰ 1000 å­—åšæ¼”ç¤º
            preview_text = fanfic_text[:2000]
            test_tokens = get_style_tokens(preview_text, blocklist)
            
            if len(test_tokens) < 20:
                st.error("æµ‹è¯•æ–‡æœ¬æœ‰æ•ˆè¯æ±‡ä¸è¶³ï¼Œæ— æ³•åˆ†æã€‚")
                st.stop()

            # --- B: è®­ç»ƒ FastText ---
            status.write("ğŸ§  è®­ç»ƒ FastText å‘é‡ç©ºé—´...")
            # è®­ç»ƒæ—¶æŠŠæµ‹è¯•æ–‡æœ¬ä¹Ÿæ”¾è¿›å»ï¼Œå»ºç«‹å…±äº«è¯­å¢ƒ
            all_docs = original_docs + [test_tokens]
            model = FastText(sentences=all_docs, vector_size=100, window=5, min_count=1, epochs=20, seed=42)
            
            # --- C: è®¡ç®—åŸºå‡†å‘é‡ ---
            def get_vec(tokens):
                vecs = [model.wv[w] for w in tokens if w in model.wv]
                return np.mean(vecs, axis=0) if vecs else np.zeros(100)

            orig_vecs = np.array([get_vec(d) for d in original_docs])
            gold_standard = np.mean(orig_vecs, axis=0) # åŸè‘—è´¨å¿ƒ
            
            # è®¡ç®—åŒäººåˆ†æ•°
            test_vec = get_vec(test_tokens)
            similarity = cosine_similarity([test_vec], [gold_standard])[0][0]
            final_score = similarity * 100
            
            status.update(label="åŸºç¡€åˆ†æå®Œæˆï¼", state="complete", expanded=False)

            # --- D: ç»“æœå±•ç¤º ---
            st.divider()
            st.subheader("ğŸ“Š åˆ†ææŠ¥å‘Š")
            
            m1, m2 = st.columns([1, 1])
            with m1:
                st.metric("æ–‡é£ç›¸ä¼¼åº¦", f"{final_score:.2f}%")
                if final_score > 85: st.success("åˆ¤å®šï¼šæåº¦è´´åˆåŸè‘—")
                elif final_score > 70: st.info("åˆ¤å®šï¼šé£æ ¼è¾ƒä¸ºæ¥è¿‘")
                else: st.warning("åˆ¤å®šï¼šä¸ªäººé£æ ¼å¼ºçƒˆ")
            
            with m2:
                # ç®€å•çš„ PCA å¯è§†åŒ–
                if len(orig_vecs) > 0:
                    try:
                        pca = PCA(n_components=2)
                        X_all = np.vstack([orig_vecs, [test_vec]])
                        X_pca = pca.fit_transform(X_all)
                        fig, ax = plt.subplots(figsize=(5, 3))
                        ax.scatter(X_pca[:-1, 0], X_pca[:-1, 1], c='lightgray', s=10, label='Original')
                        center = pca.transform([gold_standard])
                        ax.scatter(center[:,0], center[:,1], c='red', marker='*', s=150, label='Center')
                        ax.scatter(X_pca[-1, 0], X_pca[-1, 1], c='blue', marker='X', s=100, label='Fanfic')
                        ax.axis('off')
                        st.pyplot(fig)
                    except: pass

            # ==========================================
            # 4. LIME å¯è§£é‡Šæ€§åˆ†æ (The "Great Idea")
            # ==========================================
            st.divider()
            st.subheader("ğŸ” æ·±åº¦å½’å› ï¼šä¸ºä»€ä¹ˆåƒï¼Ÿ")
            st.info("LIME ç®—æ³•å°†éšæœºé®è”½æ–‡æœ¬ä¸­çš„è¯å¥ï¼Œè§‚å¯Ÿç›¸ä¼¼åº¦å˜åŒ–ï¼Œä»è€Œæ‰¾å‡ºå¯¹æ–‡é£è´¡çŒ®æœ€å¤§çš„ç‰‡æ®µã€‚")
            
            if st.button("å¼€å§‹ LIME æ·±åº¦è®¡ç®— (è€—æ—¶è¾ƒé•¿)", type="primary"):
                with st.spinner("æ­£åœ¨è¿›è¡Œæ•°åƒæ¬¡æ‰°åŠ¨é‡‡æ ·ï¼Œè¯·ç¨å€™..."):
                    
                    # 1. å®šä¹‰ LIME éœ€è¦çš„é¢„æµ‹å‡½æ•°
                    # è¾“å…¥ï¼šæ–‡æœ¬åˆ—è¡¨ [text1, text2...]
                    # è¾“å‡ºï¼šæ¦‚ç‡çŸ©é˜µ [[prob_not_sim, prob_sim], ...]
                    def predict_proba(texts):
                        results = []
                        for text in texts:
                            # æ¸…æ´—å¹¶åˆ†è¯ (ä½¿ç”¨åŒæ ·çš„é€»è¾‘)
                            t_tokens = get_style_tokens(text, blocklist)
                            if not t_tokens:
                                results.append([1.0, 0.0]) # ç©ºæ–‡æœ¬å®Œå…¨ä¸åƒ
                                continue
                            
                            # è·å–å‘é‡
                            vec = get_vec(t_tokens)
                            # è®¡ç®—ç›¸ä¼¼åº¦ (0-1)
                            sim = cosine_similarity([vec], [gold_standard])[0][0]
                            # è½¬æ¢ä¸º [ä¸ç›¸ä¼¼æ¦‚ç‡, ç›¸ä¼¼æ¦‚ç‡]
                            # ä¸ºäº†è®© LIME æ•ˆæœæ›´æ˜æ˜¾ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ sim è¿›è¡Œç¼©æ”¾ï¼Œä½†åŸå§‹å€¼ä¹Ÿè¡Œ
                            results.append([1 - sim, sim])
                        return np.array(results)

                    # 2. åˆå§‹åŒ–è§£é‡Šå™¨
                    # class_names=['Other', 'Original']
                    explainer = LimeTextExplainer(class_names=['å·®å¼‚', 'åŸè‘—é£'])

                    # 3. è¿™é‡Œçš„å…³é”®æ˜¯ï¼šLIME é»˜è®¤æŒ‰ç©ºæ ¼åˆ†è¯ã€‚
                    # ä¸ºäº†æ”¯æŒä¸­æ–‡ï¼Œæˆ‘ä»¬å…ˆæŠŠä¸­æ–‡æ–‡æœ¬å˜æˆ "è¯ è¯ è¯" çš„ç©ºæ ¼åˆ†éš”å½¢å¼
                    # è¿™æ · LIME å°±èƒ½å¤„ç†â€œè¯â€çº§åˆ«çš„è´¡çŒ®åº¦äº†
                    seg_list = jieba.cut(preview_text)
                    spaced_text = " ".join(seg_list)

                    # 4. ç”Ÿæˆè§£é‡Š
                    # num_features=10: æ˜¾ç¤ºå‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾
                    # num_samples=200: é‡‡æ ·æ¬¡æ•°ï¼Œè¶Šå¤§è¶Šå‡†ä½†è¶Šæ…¢ã€‚äº‘ç«¯å»ºè®® 200-500ã€‚
                    exp = explainer.explain_instance(
                        spaced_text, 
                        predict_proba, 
                        num_features=10, 
                        num_samples=200 
                    )

                    # 5. å±•ç¤ºç»“æœ HTML
                    # LIME ä¼šç”Ÿæˆä¸€ä¸ªéå¸¸æ¼‚äº®çš„ HTML å¯è§†åŒ–ï¼ŒåŒ…å«é«˜äº®æ–‡æœ¬
                    st.write("### è´¡çŒ®åº¦çƒ­åŠ›å›¾")
                    components.html(exp.as_html(), height=800, scrolling=True)
                    
                    # 6. æå–å…·ä½“å…³é”®è¯
                    st.write("### ğŸ† æœ€å…·â€œåŸè‘—æ„Ÿâ€çš„ç‰¹å¾è¯")
                    st.write("è¿™äº›è¯çš„å‡ºç°æ˜¾è‘—æå‡äº†æ–‡æœ¬ä¸åŸè‘—çš„ç›¸ä¼¼åº¦ï¼ˆä¸ä»…ä»…æ˜¯åè¯ï¼Œæ›´å¤šæ˜¯è¯­æ°”è¯ã€åŠ¨è¯ï¼‰ï¼š")
                    
                    top_features = exp.as_list()
                    # è¿‡æ»¤å‡ºæ­£å‘è´¡çŒ®çš„è¯
                    positive_features = [f for f in top_features if f[1] > 0]
                    
                    if positive_features:
                        feat_df = pd.DataFrame(positive_features, columns=["ç‰¹å¾è¯", "è´¡çŒ®åº¦"])
                        st.dataframe(feat_df, use_container_width=True)
                    else:
                        st.write("æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„æ­£å‘ç‰¹å¾ã€‚")

