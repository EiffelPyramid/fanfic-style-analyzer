#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import streamlit as st
import pandas as pd
import jieba
import jieba.posseg as pseg
from gensim.models import FastText
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import matplotlib.colors as mcolors
import numpy as np
import re
import os
import streamlit.components.v1 as components

from lime.lime_text import LimeTextExplainer

# ==========================================
# 0. é¡µé¢é…ç½®ä¸å­—ä½“å®‰å…¨æ£€æŸ¥
# ==========================================
st.set_page_config(page_title="æ–‡é£åˆ†æå®éªŒå®¤", layout="wide")

@st.cache_resource
def get_font_prop():
    font_path = "simhei.ttf"
    if not os.path.exists(font_path): return None
    try:
        if os.path.getsize(font_path) / (1024 * 1024) < 1: return None
        return fm.FontProperties(fname=font_path)
    except: return None

my_font_prop = get_font_prop()

# ==========================================
# 1. æ ¸å¿ƒå·¥å…·å‡½æ•°
# ==========================================

def read_content_safe(file_obj, limit=None):
    """å®‰å…¨è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        file_obj.seek(0)
        content_bytes = file_obj.read()
        text = content_bytes.decode('utf-8')
    except UnicodeDecodeError:
        try:
            text = content_bytes.decode('gbk')
        except UnicodeDecodeError:
            text = content_bytes.decode('utf-8', errors='ignore')
    if limit: return text[:limit]
    return text

def basic_clean(text):
    """åŸºç¡€æ¸…æ´—"""
    if not isinstance(text, str): return ""
    text = re.sub(r'ç¬¬.+?ç« .*', '', text)
    text = re.sub(r'Chapter.*', '', text)
    text = re.sub(r'\[\d+\]|[\u2460-\u24FF]', '', text)
    punctuation_map = {',': 'ï¼Œ', '!': 'ï¼', '?': 'ï¼Ÿ', '(': 'ï¼ˆ', ')': 'ï¼‰', ':': 'ï¼š', ';': 'ï¼›'}
    for eng_punc, chi_punc in punctuation_map.items():
        text = text.replace(eng_punc, chi_punc)
    return text

def split_sentences_custom(text):
    """
    è‡ªå®šä¹‰åˆ†å¥å‡½æ•°ï¼šä¿ç•™æ ‡ç‚¹ç¬¦å·
    """
    # ä½¿ç”¨æ­£åˆ™æŒ‰å¥å·ã€æ„Ÿå¹å·ã€é—®å·ã€æ¢è¡Œç¬¦åˆ‡åˆ†ï¼Œå¹¶ä¿ç•™åˆ†éš”ç¬¦
    # æ¨¡å¼è§£é‡Šï¼š([^ã€‚ï¼ï¼Ÿ\n]+[ã€‚ï¼ï¼Ÿ\n]?) åŒ¹é…éåˆ†éš”ç¬¦å¼€å¤´ï¼Œä»¥åˆ†éš”ç¬¦æˆ–ç»“å°¾ç»“æŸçš„ä¸²
    sents = re.split(r'([ã€‚ï¼ï¼Ÿ\n]+)', text)
    # re.split ä¼šæŠŠåˆ†éš”ç¬¦å•ç‹¬åˆ‡å‡ºæ¥ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒä»¬æ‹¼å›å»
    # ä¾‹å­ï¼š['ä½ å¥½', 'ï¼', 'å†è§', 'ã€‚', '']
    new_sents = []
    for i in range(0, len(sents) - 1, 2):
        new_sents.append(sents[i] + sents[i+1])
    if sents[-1]: new_sents.append(sents[-1])
    # è¿‡æ»¤ç©ºå¥
    return [s for s in new_sents if s.strip()]

def smart_chunking(text, min_length=300):
    """æ™ºèƒ½åˆ‡åˆ†é•¿æ–‡æœ¬ç”¨äºè®­ç»ƒ"""
    lines = text.split("\n")
    final_chunks = []
    current_chunk = ""
    for line in lines:
        line = line.strip()
        if len(line) < 2: continue
        current_chunk += line + " "
        if len(current_chunk) >= min_length:
            final_chunks.append(current_chunk)
            current_chunk = ""
    if len(current_chunk) > 50:
        final_chunks.append(current_chunk)
    return final_chunks

def get_style_tokens(text, blocklist):
    """æ–‡é£åˆ†è¯ï¼šåŸºäºåœç”¨è¯è¡¨è¿‡æ»¤"""
    text = basic_clean(text)
    words = jieba.lcut(text)
    # è¿‡æ»¤é€»è¾‘ï¼šä¿ç•™éé»‘åå•è¯ä¸”éçº¯ç©ºç™½
    return [w for w in words if w not in blocklist and not w.isspace()]

def generate_blocklist_from_files(uploaded_files):
    """è‡ªåŠ¨ç”Ÿæˆåœç”¨è¯è¡¨ï¼šå®è¯"""
    sample_text = ""
    for uploaded_file in uploaded_files:
        content = read_content_safe(uploaded_file)
        sample_text += basic_clean(content)[:50000]
    words = pseg.cut(sample_text)
    candidates = []
    target_flags = {'nr', 'ns', 'nz', 'nt', 'per', 'loc'}
    for w, f in words:
        if len(w) > 1 and f in target_flags:
            candidates.append(w)
    from collections import Counter
    blocklist = set([w for w, c in Counter(candidates).most_common(500)])
    return blocklist

def get_color_html(text, weight):
    """
    ä¼˜åŒ–ç‰ˆï¼šå¢å¼ºé¢œè‰²å¯è§†æ€§ï¼Œé€‚é…æ·±è‰²/æµ…è‰²æ¨¡å¼
    """
    # 1. åŠ¨æ€æ”¾å¤§æƒé‡
    # LIME çš„å¥å­æƒé‡é€šå¸¸è¾ƒå°ï¼Œæˆ‘ä»¬å°†å…¶æ”¾å¤§ 10 å€ï¼Œå¹¶é™åˆ¶æœ€å¤§é€æ˜åº¦ä¸º 0.7
    # é™åˆ¶ä¸º 0.7 æ˜¯ä¸ºäº†ä¿è¯æ–‡å­—ï¼ˆæ— è®ºæ˜¯é»‘å­—è¿˜æ˜¯ç™½å­—ï¼‰ä¾ç„¶æ¸…æ™°å¯è¯»
    val = abs(weight)
    if val < 0.001: return text # æƒé‡å¤ªå°ä¸æŸ“è‰²
    
    intensity = min(val * 10, 0.7) 
    
    # 2. è®¾å®šâ€œä¿åº•â€é€æ˜åº¦
    # åªè¦æœ‰æƒé‡ï¼Œè‡³å°‘ç»™ 0.15 çš„é€æ˜åº¦ï¼Œé˜²æ­¢é¢œè‰²å¤ªæµ…çœ‹ä¸è§
    intensity = max(intensity, 0.15)

    if weight > 0:
        # æ­£å‘ï¼ˆåƒåŸè‘—ï¼‰ï¼šä½¿ç”¨äº®çº¢è‰² (255, 60, 60)
        # åŸæ¥çš„ (255, 0, 0) åœ¨é»‘åº•ä¸Šå®¹æ˜“æ˜¾å¾—æš—æ²‰ï¼ŒåŠ ä¸€ç‚¹ç»¿è“åˆ†é‡ä¼šæ›´äº®
        rgba = f"rgba(255, 60, 60, {intensity})" 
    else:
        # è´Ÿå‘ï¼ˆä¸åƒåŸè‘—ï¼‰ï¼šä½¿ç”¨äº®è“è‰² (0, 160, 255)
        # çº¯è“ (0, 0, 255) åœ¨æš—å¤œæ¨¡å¼ä¸‹å‡ ä¹éšå½¢ï¼Œå¿…é¡»æé«˜ç»¿è‰²åˆ†é‡å˜æˆâ€œå¤©è“â€
        rgba = f"rgba(0, 160, 255, {intensity})"
        
    return f"<span style='background-color: {rgba}; padding: 2px 4px; border-radius: 4px;'>{text}</span>"

# ==========================================
# 2. ç½‘ç«™ç•Œé¢ UI
# ==========================================

st.title("ğŸ•µï¸â€â™‚ï¸ æ–‡é£åˆ†æå®éªŒå®¤")
st.markdown("""
ä¸Šä¼ æŸä½ä½œå®¶çš„åŸè‘—ï¼Œå†è¾“å…¥ä½ çš„åŒäººæ–‡æœ¬ï¼Œç®—æ³•å°†æ ¹æ®è™šè¯ã€å¥å¼ç­‰ï¼ˆè€Œéå‰§æƒ…å†…å®¹ï¼‰è®¡ç®—æ–‡é£ç›¸ä¼¼åº¦ï¼Œå¹¶è¾“å‡ºæœ€å…·åŸè‘—å‘³çš„å¥å­ã€‚
""")

col1, col2 = st.columns([1, 2])

with col1:
    st.header("Step 1: å»ºç«‹åŸºå‡†")
    st.info("è¯·ä¸Šä¼ åŸè‘— TXT æ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰")
    uploaded_originals = st.file_uploader("ä¸Šä¼ åŸè‘— (æ”¯æŒ .txt)", type="txt", accept_multiple_files=True)

    st.header("Step 2: è¾“å…¥æµ‹è¯•æ–‡æœ¬")
    fanfic_text = st.text_area("åœ¨æ­¤ç²˜è´´ä½ çš„åŒäººæ–‡æœ¬ï¼š", height=200, placeholder="å»ºè®®ç²˜è´´ 500 å­—ä»¥ä¸Šçš„æ®µè½...")

    start_btn = st.button("ğŸš€ å¼€å§‹æ–‡é£åˆ†æ", type="primary")

# ==========================================
# 3. ä¸»é€»è¾‘æ§åˆ¶å™¨
# ==========================================

if start_btn:
    if not uploaded_originals:
        st.error("âŒ è¯·å…ˆä¸Šä¼ åŸè‘—æ–‡ä»¶ï¼")
    elif not fanfic_text.strip():
        st.error("âŒ è¯·è¾“å…¥æµ‹è¯•æ–‡æœ¬ï¼")
    else:
        with col2:
            # === é˜¶æ®µä¸€ï¼šåŸºç¡€æ¨¡å‹æ„å»ºä¸è®¡ç®— ===
            status = st.status("æ­£åœ¨è¿›è¡Œå…¨æµç¨‹åˆ†æ...", expanded=True)
            
            # 1. é¢„å¤„ç†
            status.write("ğŸ“– æ­£åœ¨æ‰«æåŸè‘—å¹¶æ„å»ºå®è¯åœç”¨è¯è¡¨...")
            blocklist = generate_blocklist_from_files(uploaded_originals)
            status.write(f"âœ… å·²åœç”¨ {len(blocklist)} ä¸ªé«˜é¢‘ä¸“æœ‰åè¯ï¼ˆå¦‚ï¼š{list(blocklist)[:5]}...ï¼‰")
            
            # 2. æ•°æ®åˆ‡åˆ†
            status.write("âœ‚ï¸ æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ‡ç‰‡ä¸æ¸…æ´—...")
            original_docs = []
            for u_file in uploaded_originals:
                content = read_content_safe(u_file)
                chunks = smart_chunking(content)
                for chunk in chunks:
                    tokens = get_style_tokens(chunk, blocklist)
                    if len(tokens) > 50: original_docs.append(tokens)
            
            # 3. å¤„ç†åŒäººæ–‡æœ¬ (ç”¨äº FastText è®­ç»ƒçš„ Token)
            # æˆ‘ä»¬ç”¨ç¨å¾®é•¿ä¸€ç‚¹çš„æ–‡æœ¬è®­ç»ƒæ¨¡å‹ï¼Œä¿è¯è¯­å¢ƒ
            preview_text = fanfic_text[:3000] 
            test_tokens = get_style_tokens(preview_text, blocklist)
            
            if len(test_tokens) < 10:
                status.update(label="åˆ†æå¤±è´¥ï¼šæœ‰æ•ˆè¯æ±‡ä¸è¶³", state="error")
                st.stop()

            # 4. è®­ç»ƒæ¨¡å‹
            status.write("ğŸ§  æ­£åœ¨è®­ç»ƒ FastText æ–‡é£å‘é‡ç©ºé—´...")
            all_docs = original_docs + [test_tokens]
            model = FastText(sentences=all_docs, vector_size=100, window=5, min_count=1, epochs=20, seed=42)
            
            # 5. è®¡ç®—åŸºå‡†ä¸å¾—åˆ†
            def get_vec(tokens):
                vecs = [model.wv[w] for w in tokens if w in model.wv]
                return np.mean(vecs, axis=0) if vecs else np.zeros(100)

            orig_vecs = np.array([get_vec(d) for d in original_docs])
            gold_standard = np.mean(orig_vecs, axis=0) # åŸè‘—è´¨å¿ƒ
            test_vec = get_vec(test_tokens)
            
            similarity = cosine_similarity([test_vec], [gold_standard])[0][0]
            final_score = similarity * 100
            
            status.write("âœ… åŸºç¡€åˆ†æå®Œæˆï¼Œå‡†å¤‡è¿›è¡Œå¥å­å½’å› ...")

            # === é˜¶æ®µäºŒï¼šç»“æœå±•ç¤º (åŸºç¡€éƒ¨åˆ†) ===
            st.divider()
            st.subheader("ğŸ“Š ç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š")
            
            res_c1, res_c2 = st.columns([1, 1])
            
            with res_c1:
                st.metric(label="æ•´ä½“æ–‡é£ç›¸ä¼¼åº¦", value=f"{final_score:.2f}%")
                
                # è¯„è¯­é€»è¾‘ (åŒäººåœˆç‰¹ä¾›ç‰ˆ)
                if final_score > 90:
                    st.success("""
                    **åˆ¤å®šï¼šç–‘ä¼¼ä½œè€…å°å·ï¼ˆTier Sï¼‰** ğŸ˜­ **æ•‘å‘½ï¼è¿™æ˜¯å“ªä½ç¥ä»™å¤ªå¤ªä¸‹å‡¡ï¼Ÿ** è¿™ç®€ç›´å°±æ˜¯åŸè‘—ï¼è‹¥ä¸æ˜¯ä½œè€…çš„å°å·ï¼Œå»ºè®®ä¸¥æŸ¥æ˜¯å¦å·äº†å­˜ç¨¿ç¡¬ç›˜ã€‚  
                    *è¯„ä»·ï¼šç»èµå¥½ç²®ï¼Œå‚ç›´å…¥å‘ï¼Œè¯·å—æˆ‘ä¸€æ‹œï¼*
                    """)
                elif final_score > 75:
                    st.info("""
                    **åˆ¤å®šï¼šç¾å‘³ï¼ˆTier Aï¼‰** ğŸ˜‹ **å¥½ä¸€å£ç¾å‘³çš„ç²®ï¼** è™½ç„¶åœ¨ç»†èŠ‚å¤„èƒ½çœ‹å‡ºå¤ªå¤ªè‡ªå·±çš„è¡Œæ–‡ä¹ æƒ¯ï¼Œä½†æ•´ä½“è¿˜åŸåº¦æé«˜ã€‚  
                    *è¯„ä»·ï¼šæ˜¯ä¸å¯å¤šå¾—çš„ä¼˜è´¨ç²®ï¼Œè¿™å°±åŠ å…¥ä¹¦æ¶ï¼*
                    """)
                elif final_score > 60:
                    st.warning("""
                    **åˆ¤å®šï¼šè‡ªå¸¦æ»¤é•œçš„AUæ„Ÿï¼ˆTier Bï¼‰** ğŸ¤” **è¿™æ˜¯ä»€ä¹ˆå¥‡æ€ªçš„paå—ï¼Ÿ** è™½ç„¶è¿˜åœ¨åŒäººçš„èŒƒç•´é‡Œï¼Œä½†æ˜¯ç§è®¾æ¯”è¾ƒå¤šå‘¢ã€‚  
                    *è¯„ä»·ï¼šç†Ÿæ‚‰çš„é™Œç”Ÿäººï¼Œä»¿ä½›åœ¨OOCè¾¹ç¼˜è¯•æ¢ï¼ˆï¼‰*
                    """)
                else:
                    st.error("""
                    **åˆ¤å®šï¼šOOCé¢„è­¦ / çº¯å±åŸåˆ›ï¼ˆTier Cï¼‰** ğŸ˜¨ **ç¡®å®šè¿™æ˜¯åŒäººï¼Ÿ** è¿™ç‹¬ç‰¹çš„æ–‡é£å·²ç»å®Œå…¨è„±ç¦»äº†åŸè‘—çš„å¼•åŠ›åœˆï¼Œå¦‚æœä¸çœ‹è§’è‰²åï¼Œæœºå™¨è¿˜ä»¥ä¸ºè¯¯å…¥äº†éš”å£ç‰‡åœºã€‚  
                    *è¯„ä»·ï¼šè¿™æ˜¯æè‡´çš„OOCï¼Œè¿˜æ˜¯æŠ«ç€åŒäººçš®çš„åŸåˆ›å¤§ä½œï¼Ÿè¿™å¾ˆéš¾è¯„ï¼Œç¥æ‚¨å¼€å¿ƒå°±å¥½ã€‚*
                    """)
            
            with metric_col2:
                st.write("### å‘é‡ç©ºé—´æŠ•å½±")
                if len(orig_vecs) > 0:
                    try:
                        pca = PCA(n_components=2)
                        X_all = np.vstack([orig_vecs, [test_vec]])
                        X_pca = pca.fit_transform(X_all)
                        n_orig = len(orig_vecs)

                        fig, ax = plt.subplots(figsize=(6, 4))

                        # ã€å…³é”®ä¿®æ”¹1ã€‘è®¾ç½®èƒŒæ™¯é€æ˜
                        fig.patch.set_alpha(0.0)  # å°†å›¾ç‰‡åº•è‰²è®¾ä¸ºé€æ˜
                        ax.patch.set_alpha(0.0)   # å°†ç»˜å›¾åŒºåº•è‰²è®¾ä¸ºé€æ˜

                        # ç»˜å›¾éƒ¨åˆ†
                        ax.scatter(X_pca[:n_orig, 0], X_pca[:n_orig, 1], c='lightgray', s=10, alpha=0.5, label='åŸè‘—åˆ‡ç‰‡')
                        center = pca.transform([gold_standard])
                        ax.scatter(center[:,0], center[:,1], c='red', marker='*', s=200, label='åŸè‘—åŸºå‡†')
                        ax.scatter(X_pca[n_orig:, 0], X_pca[n_orig:, 1], c='blue', s=80, marker='X', label='ä½ çš„æ–‡æœ¬')

                        # ã€å®‰å…¨ç»˜å›¾ã€‘åªæœ‰å½“å­—ä½“å¯¹è±¡æœ‰æ•ˆæ—¶ï¼Œæ‰åº”ç”¨å­—ä½“
                        if my_font_prop:
                            # ã€å…³é”®ä¿®æ”¹2ã€‘frameon=False å»é™¤å›¾ä¾‹çš„è¾¹æ¡†
                            ax.legend(prop=my_font_prop, frameon=False) 
                            ax.set_title("æ–‡é£è½ç‚¹åˆ†å¸ƒ", fontproperties=my_font_prop)
                        else:
                            ax.legend(frameon=False)
                            ax.set_title("Style Distribution (Font Missing)")

                        # å…³é—­åæ ‡è½´ï¼ˆè¿™ä¸€æ­¥æœ¬èº«å°±å»é™¤äº†å¤§éƒ¨åˆ†è¾¹æ¡†ï¼‰
                        ax.axis('off')

                        # æ¸²æŸ“å›¾ç‰‡
                        st.pyplot(fig)
                    except Exception as e:
                        st.error(f"ç»˜å›¾å‡ºé”™: {e}")
                        
                        
            # === é˜¶æ®µä¸‰ï¼šå¥å­çº§ LIME è¿›é˜¶åˆ†æ ===
            st.divider()
            st.subheader("ğŸ” æ·±åº¦å½’å› ï¼šå“ªäº›å¥å­æœ€åƒåŸè‘—ï¼Ÿ")
            st.info("æ­£åœ¨é€å¥åˆ†ææ–‡é£è´¡çŒ®åº¦ï¼ˆçº¢è‰²=åŠ åˆ†é¡¹ï¼Œè“è‰²=å‡åˆ†é¡¹ï¼‰...")
            
            # --- æ ¸å¿ƒé»‘ç§‘æŠ€ï¼šå¥å­çº§ LIME ---
            # 1. å°†æ–‡æœ¬æ‹†åˆ†æˆå¥å­åˆ—è¡¨
            sentences_list = split_sentences_custom(preview_text)
            
            # 2. æ„é€ â€œä»£ç†æ–‡æœ¬â€ï¼šç”¨ç´¢å¼•å· "0 1 2 3" ä»£æ›¿å®é™…å¥å­ä¼ ç»™ LIME
            # è¿™æ · LIME å°±ä¼šä»¥ä¸º "0" æ˜¯ä¸€ä¸ªè¯ï¼Œå…¶å®å®ƒæ˜¯ç¬¬0å¥
            wrapped_text = " ".join([str(i) for i in range(len(sentences_list))])
            
            # 3. å®šä¹‰é¢„æµ‹å‡½æ•°ï¼šLIME ä¼šä¼ è¿›æ¥ ["0 2 3", "1 4"] è¿™æ ·çš„ç´¢å¼•ç»„åˆ
            # æˆ‘ä»¬éœ€è¦æŠŠå®ƒä»¬è¿˜åŸæˆå¥å­ï¼Œå†ç®—ç›¸ä¼¼åº¦
            def sentence_predict_proba(str_indices_list):
                results = []
                for str_indices in str_indices_list:
                    # è¿˜åŸå¥å­
                    indices = [int(i) for i in str_indices.split()]
                    # æ‹¼æ¥æˆæ–‡æœ¬
                    reconstructed_text = "".join([sentences_list[i] for i in indices])
                    
                    # ç®—åˆ†
                    t_tokens = get_style_tokens(reconstructed_text, blocklist)
                    if not t_tokens:
                        results.append([1.0, 0.0]) # ç©ºæ–‡æœ¬ä¸åƒ
                        continue
                    
                    vec = get_vec(t_tokens)
                    sim = cosine_similarity([vec], [gold_standard])[0][0]
                    
                    # æ”¾å¤§å·®å¼‚ä»¥ä¾¿å¯è§†åŒ–
                    sim_scaled = sim ** 3
                    results.append([1 - sim_scaled, sim_scaled])
                return np.array(results)

            try:
                # 4. åˆå§‹åŒ–è§£é‡Šå™¨
                explainer = LimeTextExplainer(class_names=['å·®å¼‚', 'åŸè‘—é£'])
                
                # 5. ç”Ÿæˆè§£é‡Š (num_features=æ‰€æœ‰å¥å­)
                # num_samples å¯ä»¥è°ƒä½ä¸€ç‚¹æé«˜é€Ÿåº¦ï¼Œæ¯”å¦‚ 100-200
                exp = explainer.explain_instance(
                    wrapped_text, 
                    sentence_predict_proba, 
                    num_features=len(sentences_list), 
                    num_samples=150 
                )
                
                # 6. è·å–æƒé‡ï¼šæ ¼å¼ä¸º [('3', 0.12), ('0', -0.05)...]
                weights = exp.as_list()
                # è½¬æ¢æˆå­—å…¸: {å¥å­çš„Index: æƒé‡}
                weight_map = {int(k): v for k, v in weights}
                
                # === ç»“æœå±•ç¤º A: å…¨æ–‡çƒ­åŠ›å›¾ ===
                st.write("### ğŸ“œ å…¨æ–‡æ–‡é£çƒ­åŠ›å›¾")
                st.caption("çº¢è‰²è¶Šæ·±ä»£è¡¨è¯¥å¥è¶Šæ¥è¿‘åŸè‘—æ–‡é£ï¼›è“è‰²ä»£è¡¨è¯¥å¥ä¸åŸè‘—å·®å¼‚è¾ƒå¤§ã€‚")
                
                html_parts = []
                for idx, sentence in enumerate(sentences_list):
                    weight = weight_map.get(idx, 0)
                    html_parts.append(get_color_html(sentence, weight))
                
                # æ‹¼æ¥å¹¶æ˜¾ç¤º
                full_html = f"<div style='line-height: 1.8; font-family: serif; padding: 15px; border: 1px solid #ddd; border-radius: 5px;'>{''.join(html_parts)}</div>"
                st.markdown(full_html, unsafe_allow_html=True)
                
                # === ç»“æœå±•ç¤º B: æœ€å…·è´¡çŒ®åº¦çš„å¥å­æ’è¡Œ ===
                st.write("### ğŸ† æœ€å…·â€œåŸè‘—å‘³â€çš„å¥å­ TOP 5")
                # æ’åºï¼šæƒé‡ä»å¤§åˆ°å°
                sorted_indices = sorted(weight_map.keys(), key=lambda k: weight_map[k], reverse=True)
                
                top_sentences_data = []
                for idx in sorted_indices[:5]:
                    if weight_map[idx] > 0: # åªæ˜¾ç¤ºæ­£å‘è´¡çŒ®
                        top_sentences_data.append({
                            "æ’å": len(top_sentences_data) + 1,
                            "å¥å­å†…å®¹": sentences_list[idx],
                            "è´¡çŒ®åº¦å¾—åˆ†": f"{weight_map[idx]:.4f}"
                        })
                
                if top_sentences_data:
                    st.table(pd.DataFrame(top_sentences_data).set_index("æ’å"))
                else:
                    st.write("æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„æ­£å‘ç‰¹å¾å¥å­ã€‚")

                status.update(label="å…¨æµç¨‹åˆ†æåœ†æ»¡å®Œæˆï¼", state="complete", expanded=False)

            except Exception as e:
                st.error(f"LIME åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
                status.update(label="åˆ†æä¸­æ–­", state="error")

