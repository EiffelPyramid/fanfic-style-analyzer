#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import streamlit as st
import pandas as pd
import jieba
import jieba.posseg as pseg
from gensim.models import FastText
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import re
import os

# ==========================================
# 0. é¡µé¢é…ç½®ä¸å­—ä½“å¤„ç†
# ==========================================
st.set_page_config(page_title="æ–‡é£æŒ‡çº¹åˆ†æå®éªŒå®¤", layout="wide")

# è§£å†³ç»˜å›¾ä¸­æ–‡ä¹±ç ï¼ˆè‡ªåŠ¨ä¸‹è½½å­—ä½“ï¼‰
@st.cache_resource
def get_font():
    font_path = "simhei.ttf"
    if not os.path.exists(font_path):
        os.system('wget -O simhei.ttf "https://www.wfonts.com/download/data/2014/06/01/simhei/simhei.ttf"')
    return fm.FontProperties(fname=font_path)

my_font = get_font()
plt.rcParams['font.family'] = my_font.get_name()

# ==========================================
# 1. æ ¸å¿ƒç®—æ³•å‡½æ•° (å¤ç”¨ä¹‹å‰çš„é€»è¾‘)
# ==========================================

def basic_clean(text):
    """åŸºç¡€æ¸…æ´—ï¼šå»ç« èŠ‚å¤´ã€ç»Ÿä¸€æ ‡ç‚¹"""
    if not isinstance(text, str): return ""
    text = re.sub(r'ç¬¬.+?ç« .*', '', text)
    text = re.sub(r'Chapter.*', '', text)
    text = re.sub(r'\[\d+\]|[\u2460-\u24FF]', '', text)
    punctuation_map = {',': 'ï¼Œ', '!': 'ï¼', '?': 'ï¼Ÿ', '(': 'ï¼ˆ', ')': 'ï¼‰', ':': 'ï¼š', ';': 'ï¼›'}
    for eng_punc, chi_punc in punctuation_map.items():
        text = text.replace(eng_punc, chi_punc)
    return text

def smart_chunking(text, min_length=300):
    """æ™ºèƒ½åˆ‡åˆ†ï¼šå°†æ–‡æœ¬åˆ‡åˆ†ä¸ºé•¿æ®µè½"""
    lines = text.split("\n")
    final_chunks = []
    current_chunk = ""
    for line in lines:
        line = line.strip()
        if len(line) < 2: continue
        current_chunk += line + " "
        if len(current_chunk) >= min_length:
            final_chunks.append(current_chunk)
            current_chunk = ""
    if len(current_chunk) > 50:
        final_chunks.append(current_chunk)
    return final_chunks

def get_style_tokens(text, blocklist):
    """æ–‡é£åˆ†è¯ï¼šåŸºäºé»‘åå•è¿‡æ»¤"""
    text = basic_clean(text)
    words = jieba.lcut(text)
    # æ ¸å¿ƒï¼šä¿ç•™ä¸åœ¨é»‘åå•é‡Œçš„è¯ï¼ˆä¿ç•™è™šè¯ã€æ ‡ç‚¹ã€æ™®é€šåŠ¨è¯ï¼‰
    return [w for w in words if w not in blocklist and not w.isspace()]

def generate_blocklist_from_files(uploaded_files):
    """ä»ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡ä¸­è‡ªåŠ¨ç”Ÿæˆé»‘åå•"""
    sample_text = ""
    # è¯»å–æ‰€æœ‰åŸè‘—çš„å‰ 50000 å­—
    for uploaded_file in uploaded_files:
        # æŒ‡é’ˆå½’é›¶ï¼Œé˜²æ­¢äºŒæ¬¡è¯»å–ä¸ºç©º
        uploaded_file.seek(0)
        content = uploaded_file.read().decode('utf-8', errors='ignore')
        sample_text += basic_clean(content)[:50000]
    
    words = pseg.cut(sample_text)
    candidates = []
    target_flags = {'nr', 'ns', 'nz', 'nt', 'per', 'loc'}
    
    for w, f in words:
        if len(w) > 1 and f in target_flags:
            candidates.append(w)
            
    # æˆªå– Top 500 é«˜é¢‘å®ä½“
    from collections import Counter
    blocklist = set([w for w, c in Counter(candidates).most_common(500)])
    return blocklist

# ==========================================
# 2. ç½‘ç«™ç•Œé¢ UI
# ==========================================

st.title("ğŸ•µï¸â€â™‚ï¸ æ–‡é£æŒ‡çº¹åˆ†æå®éªŒå®¤")
st.markdown("""
è¿™æ˜¯ä¸€ä¸ªåŸºäº **FastText** å’Œ **Stylometry (æ–‡ä½“å­¦)** çš„åˆ†æå·¥å…·ã€‚
ä¸Šä¼ æŸä½ä½œå®¶çš„åŸè‘—ï¼ˆå¦‚ã€Šç›—å¢“ç¬”è®°ã€‹ï¼‰ï¼Œå†è¾“å…¥ä½ çš„åŒäººæ–‡æœ¬ï¼Œç®—æ³•å°†è‡ªåŠ¨å‰¥ç¦»â€œå†…å®¹â€ï¼Œä»…æ ¹æ®â€œæ–‡é£â€è®¡ç®—ç›¸ä¼¼åº¦ã€‚
""")

col1, col2 = st.columns([1, 2])

with col1:
    st.header("Step 1: å»ºç«‹åŸºå‡†")
    st.info("è¯·ä¸Šä¼ åŸè‘— TXT æ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰ã€‚ç³»ç»Ÿå°†è‡ªåŠ¨å­¦ä¹ å…¶æ–‡é£å¹¶å»ºç«‹é»‘åå•ã€‚")
    uploaded_originals = st.file_uploader("ä¸Šä¼ åŸè‘— (æ”¯æŒ .txt)", type="txt", accept_multiple_files=True)

    st.header("Step 2: è¾“å…¥æµ‹è¯•æ–‡æœ¬")
    fanfic_text = st.text_area("åœ¨æ­¤ç²˜è´´ä½ çš„åŒäºº/æµ‹è¯•æ–‡æœ¬ï¼š", height=200, placeholder="æŠŠè¦æµ‹è¯•çš„å°è¯´ç‰‡æ®µç²˜è´´åœ¨è¿™é‡Œ...")

    start_btn = st.button("ğŸš€ å¼€å§‹æ–‡é£åˆ†æ", type="primary")

# ==========================================
# 3. ä¸»é€»è¾‘æ§åˆ¶å™¨
# ==========================================

if start_btn:
    if not uploaded_originals:
        st.error("è¯·å…ˆä¸Šä¼ åŸè‘—æ–‡ä»¶ï¼")
    elif not fanfic_text.strip():
        st.error("è¯·è¾“å…¥æµ‹è¯•æ–‡æœ¬ï¼")
    else:
        with col2:
            with st.status("æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æ...", expanded=True) as status:
                
                # --- é˜¶æ®µ A: é¢„å¤„ç†åŸè‘— ---
                st.write("ğŸ“– æ­£åœ¨è¯»å–åŸè‘—å¹¶ç”Ÿæˆå®ä½“é»‘åå•...")
                blocklist = generate_blocklist_from_files(uploaded_originals)
                st.write(f"âœ… å·²è‡ªåŠ¨å±è”½ {len(blocklist)} ä¸ªé«˜é¢‘ä¸“æœ‰åè¯ï¼ˆå¦‚ï¼š{list(blocklist)[:5]}...ï¼‰")

                st.write("âœ‚ï¸ æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†æ®µä¸å»å™ª...")
                original_docs = []
                for u_file in uploaded_originals:
                    u_file.seek(0)
                    content = u_file.read().decode('utf-8', errors='ignore')
                    chunks = smart_chunking(content)
                    for chunk in chunks:
                        tokens = get_style_tokens(chunk, blocklist)
                        if len(tokens) > 50:
                            original_docs.append(tokens)
                
                # --- é˜¶æ®µ B: å¤„ç†æµ‹è¯•æ–‡æœ¬ ---
                test_docs = []
                test_chunks = smart_chunking(fanfic_text, min_length=200) # æµ‹è¯•æ–‡æœ¬ä¹Ÿå¯ä»¥åˆ‡ç‰‡
                for chunk in test_chunks:
                    tokens = get_style_tokens(chunk, blocklist)
                    if len(tokens) > 50:
                        test_docs.append(tokens)
                
                if not test_docs:
                    st.error("æµ‹è¯•æ–‡æœ¬å¤ªçŸ­æˆ–æœ‰æ•ˆè¯æ±‡å¤ªå°‘ï¼Œæ— æ³•åˆ†æã€‚")
                    st.stop()

                # --- é˜¶æ®µ C: è®­ç»ƒæ¨¡å‹ ---
                st.write("ğŸ§  æ­£åœ¨è®­ç»ƒ FastText æ–‡é£æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
                # æ··åˆè®­ç»ƒ
                all_docs = original_docs + test_docs
                model = FastText(sentences=all_docs, vector_size=100, window=5, min_count=1, epochs=20, seed=42)
                
                # --- é˜¶æ®µ D: è®¡ç®—ç›¸ä¼¼åº¦ ---
                def get_vec(tokens):
                    vecs = [model.wv[w] for w in tokens if w in model.wv]
                    return np.mean(vecs, axis=0) if vecs else np.zeros(100)

                orig_vecs = np.array([get_vec(d) for d in original_docs])
                test_vecs = np.array([get_vec(d) for d in test_docs])
                
                gold_standard = np.mean(orig_vecs, axis=0) # åŸè‘—è´¨å¿ƒ
                test_centroid = np.mean(test_vecs, axis=0) # æµ‹è¯•æ–‡æœ¬è´¨å¿ƒï¼ˆå¦‚æœæœ‰å¤šæ®µï¼‰
                
                similarity = cosine_similarity([test_centroid], [gold_standard])[0][0]
                final_score = similarity * 100
                
                status.update(label="åˆ†æå®Œæˆï¼", state="complete", expanded=False)

            # ==========================================
            # 4. ç»“æœå±•ç¤º
            # ==========================================
            st.divider()
            st.subheader("åˆ†æç»“æœ")
            
            # ä»ªè¡¨ç›˜æ ·å¼æ˜¾ç¤ºåˆ†æ•°
            metric_col1, metric_col2 = st.columns(2)
            with metric_col1:
                st.metric(label="æ–‡é£ç›¸ä¼¼åº¦", value=f"{final_score:.2f}%")
                if final_score > 90:
                    st.success("åˆ¤å®šï¼šæåº¦ç›¸ä¼¼ï¼ˆå¯èƒ½æ˜¯çœŸçˆ±ç²‰æˆ–é«˜åº¦æ¨¡ä»¿ï¼‰")
                elif final_score > 75:
                    st.info("åˆ¤å®šï¼šé£æ ¼æ¥è¿‘ï¼ˆæŠ“ä½äº†è¯­æ„Ÿï¼Œä½†ç•¥æœ‰å·®å¼‚ï¼‰")
                else:
                    st.warning("åˆ¤å®šï¼šå·®å¼‚æ˜¾è‘—ï¼ˆå¯èƒ½æ˜¯ç”±äºOOCæˆ–ä¸ªäººé£æ ¼å¼ºçƒˆï¼‰")

            # å¯è§†åŒ–ç»˜å›¾
            with metric_col2:
                st.write("### å‘é‡ç©ºé—´æŠ•å½±")
                if len(orig_vecs) > 0:
                    # PCA é™ç»´
                    pca = PCA(n_components=2)
                    X_all = np.vstack([orig_vecs, test_vecs])
                    X_pca = pca.fit_transform(X_all)
                    
                    n_orig = len(orig_vecs)
                    
                    fig, ax = plt.subplots(figsize=(6, 4))
                    # åŸè‘—ç‚¹ï¼ˆèƒŒæ™¯ï¼‰
                    ax.scatter(X_pca[:n_orig, 0], X_pca[:n_orig, 1], c='lightgray', s=10, alpha=0.5, label='åŸè‘—åˆ‡ç‰‡')
                    # åŸè‘—ä¸­å¿ƒ
                    center = pca.transform([gold_standard])
                    ax.scatter(center[:,0], center[:,1], c='red', marker='*', s=200, label='åŸè‘—åŸºå‡†')
                    # æµ‹è¯•æ–‡æœ¬ç‚¹
                    ax.scatter(X_pca[n_orig:, 0], X_pca[n_orig:, 1], c='blue', s=80, marker='X', label='ä½ çš„æ–‡æœ¬')
                    
                    ax.legend(prop=my_font)
                    ax.set_title("æ–‡é£è½ç‚¹åˆ†å¸ƒ", fontproperties=my_font)
                    ax.axis('off') # å»æ‰åæ ‡è½´æ›´ç¾è§‚
                    st.pyplot(fig)

