#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import streamlit as st
import pandas as pd
import jieba
import jieba.posseg as pseg
from gensim.models import FastText
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import re
import os
import streamlit.components.v1 as components

# å¼•å…¥ LIME åº“
from lime.lime_text import LimeTextExplainer

# ==========================================
# 0. é¡µé¢é…ç½®ä¸å­—ä½“å®‰å…¨æ£€æŸ¥
# ==========================================
st.set_page_config(page_title="æ–‡é£æŒ‡çº¹åˆ†æå®éªŒå®¤ (ç»ˆæç‰ˆ)", layout="wide")

@st.cache_resource
def get_font_prop():
    font_path = "simhei.ttf"
    # ä¸¥æ ¼æ£€æŸ¥å­—ä½“æ–‡ä»¶
    if not os.path.exists(font_path): return None
    try:
        if os.path.getsize(font_path) / (1024 * 1024) < 1: return None
        return fm.FontProperties(fname=font_path)
    except: return None

my_font_prop = get_font_prop()

# ==========================================
# 1. æ ¸å¿ƒå·¥å…·å‡½æ•°
# ==========================================

def read_content_safe(file_obj, limit=None):
    """å®‰å…¨è¯»å–æ–‡ä»¶å†…å®¹ (å…¼å®¹ UTF-8 å’Œ GBK)"""
    try:
        file_obj.seek(0)
        content_bytes = file_obj.read()
        text = content_bytes.decode('utf-8')
    except UnicodeDecodeError:
        try:
            text = content_bytes.decode('gbk')
        except UnicodeDecodeError:
            text = content_bytes.decode('utf-8', errors='ignore')
    if limit: return text[:limit]
    return text

def basic_clean(text):
    """åŸºç¡€æ¸…æ´—ï¼šå»ç« èŠ‚å¤´ã€ç»Ÿä¸€æ ‡ç‚¹"""
    if not isinstance(text, str): return ""
    text = re.sub(r'ç¬¬.+?ç« .*', '', text)
    text = re.sub(r'Chapter.*', '', text)
    text = re.sub(r'\[\d+\]|[\u2460-\u24FF]', '', text)
    punctuation_map = {',': 'ï¼Œ', '!': 'ï¼', '?': 'ï¼Ÿ', '(': 'ï¼ˆ', ')': 'ï¼‰', ':': 'ï¼š', ';': 'ï¼›'}
    for eng_punc, chi_punc in punctuation_map.items():
        text = text.replace(eng_punc, chi_punc)
    return text

def smart_chunking(text, min_length=300):
    """æ™ºèƒ½åˆ†æ®µ"""
    lines = text.split("\n")
    final_chunks = []
    current_chunk = ""
    for line in lines:
        line = line.strip()
        if len(line) < 2: continue
        current_chunk += line + " "
        if len(current_chunk) >= min_length:
            final_chunks.append(current_chunk)
            current_chunk = ""
    if len(current_chunk) > 50:
        final_chunks.append(current_chunk)
    return final_chunks

def get_style_tokens(text, blocklist):
    """æ–‡é£åˆ†è¯ï¼šåŸºäºé»‘åå•è¿‡æ»¤å†…å®¹è¯"""
    text = basic_clean(text)
    words = jieba.lcut(text)
    return [w for w in words if w not in blocklist and not w.isspace()]

def generate_blocklist_from_files(uploaded_files):
    """è‡ªåŠ¨ç”Ÿæˆå†…å®¹è¯é»‘åå•"""
    sample_text = ""
    for uploaded_file in uploaded_files:
        content = read_content_safe(uploaded_file)
        sample_text += basic_clean(content)[:50000]
    words = pseg.cut(sample_text)
    candidates = []
    target_flags = {'nr', 'ns', 'nz', 'nt', 'per', 'loc'}
    for w, f in words:
        if len(w) > 1 and f in target_flags:
            candidates.append(w)
    from collections import Counter
    blocklist = set([w for w, c in Counter(candidates).most_common(500)])
    return blocklist

# ==========================================
# 2. ç½‘ç«™ç•Œé¢ UI
# ==========================================

st.title("ğŸ•µï¸â€â™‚ï¸ æ–‡é£æŒ‡çº¹åˆ†æå®éªŒå®¤ (Pro Plus)")
st.markdown("""
æœ¬ç³»ç»Ÿé€šè¿‡ **FastText** å‘é‡åŒ–ä¸ **LIME** å¯è§£é‡Šæ€§æ¨¡å‹ï¼Œå¯¹æ–‡æœ¬è¿›è¡ŒåŒé‡åˆ†æï¼š
1.  **æ–‡é£æŒ‡çº¹æ¯”å¯¹**ï¼šå‰¥ç¦»å‰§æƒ…å†…å®¹ï¼Œä»…é€šè¿‡è™šè¯ã€å¥å¼ç­‰â€œæŒ‡çº¹â€è®¡ç®—æ•´ä½“ç›¸ä¼¼åº¦ã€‚
2.  **æ·±åº¦å½’å› è§£é‡Š**ï¼šé«˜äº®æ˜¾ç¤ºæ–‡æœ¬ä¸­å“ªäº›è¯å¥å¯¹â€œåƒåŸè‘—â€è´¡çŒ®æœ€å¤§ã€‚
""")

col1, col2 = st.columns([1, 2])

with col1:
    st.header("Step 1: å»ºç«‹åŸºå‡†")
    uploaded_originals = st.file_uploader("ä¸Šä¼ åŸè‘— (æ”¯æŒ .txt)", type="txt", accept_multiple_files=True)
    
    st.header("Step 2: è¾“å…¥æµ‹è¯•æ–‡æœ¬")
    fanfic_text = st.text_area("åœ¨æ­¤ç²˜è´´åŒäºº/æµ‹è¯•æ–‡æœ¬ï¼š", height=250, placeholder="å»ºè®®ç²˜è´´ 500 å­—ä»¥ä¸Šçš„æ®µè½...")

    # è¿™é‡Œçš„æŒ‰é’®ä¸€æ—¦ç‚¹å‡»ï¼Œå°±ä¼šè§¦å‘ä¸‹é¢çš„æ‰€æœ‰é€»è¾‘
    start_btn = st.button("ğŸš€ ä¸€é”®å¼€å¯å…¨æµç¨‹åˆ†æ", type="primary")

# ==========================================
# 3. ä¸»é€»è¾‘æ§åˆ¶å™¨ (åˆå¹¶äº†åŸºç¡€ä¸è¿›é˜¶)
# ==========================================

if start_btn:
    if not uploaded_originals:
        st.error("âŒ è¯·å…ˆä¸Šä¼ åŸè‘—æ–‡ä»¶ï¼")
    elif not fanfic_text.strip():
        st.error("âŒ è¯·è¾“å…¥æµ‹è¯•æ–‡æœ¬ï¼")
    else:
        with col2:
            # === é˜¶æ®µä¸€ï¼šåŸºç¡€æ¨¡å‹æ„å»ºä¸è®¡ç®— ===
            with st.status("æ­£åœ¨è¿›è¡Œå…¨æµç¨‹åˆ†æ...", expanded=True) as status:
                
                # 1. é¢„å¤„ç†
                status.write("ğŸ“– æ­£åœ¨æ‰«æåŸè‘—å¹¶æ„å»ºå»å™ªé»‘åå•...")
                blocklist = generate_blocklist_from_files(uploaded_originals)
                status.write(f"âœ… å·²å±è”½ {len(blocklist)} ä¸ªé«˜é¢‘ä¸“æœ‰åè¯ï¼ˆå¦‚ï¼š{list(blocklist)[:3]}...ï¼‰")
                
                # 2. æ•°æ®åˆ‡åˆ†
                status.write("âœ‚ï¸ æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ‡ç‰‡ä¸æ¸…æ´—...")
                original_docs = []
                for u_file in uploaded_originals:
                    content = read_content_safe(u_file)
                    chunks = smart_chunking(content)
                    for chunk in chunks:
                        tokens = get_style_tokens(chunk, blocklist)
                        if len(tokens) > 50: original_docs.append(tokens)
                
                # å¤„ç†åŒäººæ–‡æœ¬
                preview_text = fanfic_text[:2000] # å–å‰2000å­—åšæ·±åº¦åˆ†æ
                test_tokens = get_style_tokens(preview_text, blocklist)
                
                if len(test_tokens) < 20:
                    status.update(label="åˆ†æå¤±è´¥ï¼šæœ‰æ•ˆè¯æ±‡ä¸è¶³", state="error")
                    st.stop()

                # 3. è®­ç»ƒæ¨¡å‹
                status.write("ğŸ§  æ­£åœ¨è®­ç»ƒ FastText æ–‡é£å‘é‡ç©ºé—´...")
                all_docs = original_docs + [test_tokens]
                model = FastText(sentences=all_docs, vector_size=100, window=5, min_count=1, epochs=20, seed=42)
                
                # 4. è®¡ç®—ç›¸ä¼¼åº¦
                def get_vec(tokens):
                    vecs = [model.wv[w] for w in tokens if w in model.wv]
                    return np.mean(vecs, axis=0) if vecs else np.zeros(100)

                orig_vecs = np.array([get_vec(d) for d in original_docs])
                gold_standard = np.mean(orig_vecs, axis=0) # åŸè‘—è´¨å¿ƒ
                test_vec = get_vec(test_tokens)
                
                similarity = cosine_similarity([test_vec], [gold_standard])[0][0]
                final_score = similarity * 100
                
                status.write("âœ… åŸºç¡€åˆ†æå®Œæˆï¼")
                status.update(label="ç¬¬ä¸€é˜¶æ®µåˆ†æå®Œæˆï¼Œæ­£åœ¨è¿›è¡Œ LIME æ·±åº¦å½’å› ...", state="running", expanded=True)

                # === é˜¶æ®µäºŒï¼šç»“æœå±•ç¤º (åŸºç¡€éƒ¨åˆ†) ===
                # è¿™é‡Œå°±æ˜¯ä½ å¸Œæœ›â€œä¿ç•™å‰ä¸€æ¡è¾“å‡ºâ€çš„åœ°æ–¹ï¼Œæˆ‘æŠŠå®ƒæ”¾å›æ¥äº†
                
                st.divider()
                st.subheader("ğŸ“Š åŸºç¡€åˆ†ææŠ¥å‘Š")
                
                res_c1, res_c2 = st.columns([1, 1])
                
                with res_c1:
                    st.metric(label="æ•´ä½“æ–‡é£ç›¸ä¼¼åº¦", value=f"{final_score:.2f}%")
                    
                    # è¯¦ç»†è¯„è¯­é€»è¾‘ (æ¢å¤ä½ å–œæ¬¢çš„æ–‡å­—è¯´æ˜)
                    if final_score > 90:
                        st.success("""
                        **åˆ¤å®šï¼šæåº¦ç›¸ä¼¼ï¼ˆTier Sï¼‰**
                        è¿™æ®µæ–‡æœ¬åœ¨è™šè¯ä½¿ç”¨ã€å¥å¼èŠ‚å¥å’Œç”¨è¯ä¹ æƒ¯ä¸Šä¸åŸè‘—é«˜åº¦ä¸€è‡´ã€‚
                        æœºå™¨è®¤ä¸ºè¿™ææœ‰å¯èƒ½æ˜¯åŸä½œè€…æœ¬äººæˆ–æå…¶èµ„æ·±çš„æ¨¡ä»¿è€…æ‰€å†™ã€‚
                        """)
                    elif final_score > 75:
                        st.info("""
                        **åˆ¤å®šï¼šé£æ ¼æ¥è¿‘ï¼ˆTier Aï¼‰**
                        æ–‡æœ¬æŠ“ä½äº†åŸè‘—çš„è¯­æ„Ÿç‰¹å¾ï¼Œä½†åœ¨éƒ¨åˆ†ç»†èŠ‚å¤„ç†ä¸Šä»æœ‰ä¸ªäººè‰²å½©ã€‚
                        è¿™æ˜¯ä¸€ä¸ªéå¸¸ä¼˜ç§€çš„åŒäººåˆ›ä½œï¼Œè¯»èµ·æ¥å¾ˆæœ‰â€œé‚£å‘³å„¿â€ã€‚
                        """)
                    elif final_score > 60:
                        st.warning("""
                        **åˆ¤å®šï¼šç•¥æœ‰å·®å¼‚ï¼ˆTier Bï¼‰**
                        è™½ç„¶å±äºåŒäººèŒƒç•´ï¼Œä½†ä½œè€…ä¿ç•™äº†å¼ºçƒˆçš„ä¸ªäººå™è¿°é£æ ¼ã€‚
                        æ–‡é£ä¸åŸè‘—æœ‰æ˜æ˜¾åŒºåˆ«ï¼ˆå¯èƒ½æ˜¯OOCæˆ–AUè®¾å®šå¯¼è‡´ï¼‰ã€‚
                        """)
                    else:
                        st.error("""
                        **åˆ¤å®šï¼šå·®å¼‚æ˜¾è‘—ï¼ˆTier Cï¼‰**
                        æœºå™¨éš¾ä»¥è¯†åˆ«å‡ºè¿™æ˜¯åŸºäºåŸè‘—çš„ä»¿å†™ã€‚è¿™å¯èƒ½æ˜¯ä¸€ç¯‡å®Œå…¨æ¶ç©ºçš„ç°ä»£æ–‡ï¼Œ
                        æˆ–è€…ä½œè€…çš„å†™ä½œä¹ æƒ¯ä¸åŸè‘—å¤§ç›¸å¾„åº­ã€‚
                        """)

                with res_c2:
                    st.write("**å‘é‡ç©ºé—´æŠ•å½± (PCA)**")
                    if len(orig_vecs) > 0:
                        try:
                            pca = PCA(n_components=2)
                            X_all = np.vstack([orig_vecs, [test_vec]])
                            X_pca = pca.fit_transform(X_all)
                            
                            fig, ax = plt.subplots(figsize=(6, 4))
                            # åŸè‘—ç‚¹ï¼ˆèƒŒæ™¯ï¼‰
                            ax.scatter(X_pca[:-1, 0], X_pca[:-1, 1], c='lightgray', s=15, alpha=0.6, label='åŸè‘—åˆ‡ç‰‡')
                            # åŸè‘—ä¸­å¿ƒ
                            center = pca.transform([gold_standard])
                            ax.scatter(center[:,0], center[:,1], c='red', marker='*', s=200, label='åŸè‘—åŸºå‡†')
                            # æµ‹è¯•æ–‡æœ¬ç‚¹
                            ax.scatter(X_pca[-1, 0], X_pca[-1, 1], c='blue', s=100, marker='X', edgecolors='white', label='ä½ çš„æ–‡æœ¬')
                            
                            # å­—ä½“å®‰å…¨è®¾ç½®
                            if my_font_prop:
                                ax.legend(prop=my_font_prop)
                                ax.set_title("æ–‡é£è½ç‚¹åˆ†å¸ƒå›¾", fontproperties=my_font_prop)
                            else:
                                ax.legend()
                                ax.set_title("Style Distribution")
                                
                            ax.axis('off') # å»æ‰åæ ‡è½´æ›´ç¾è§‚
                            st.pyplot(fig)
                        except Exception as e:
                            st.error(f"ç»˜å›¾é”™è¯¯: {e}")

                # === é˜¶æ®µä¸‰ï¼šLIME è¿›é˜¶åˆ†æ (è‡ªåŠ¨ç»§ç»­æ‰§è¡Œ) ===
                st.divider()
                st.subheader("ğŸ” è¿›é˜¶åˆ†æï¼šLIME å¯è§£é‡Šæ€§å½’å› ")
                st.info("AI æ­£åœ¨é€šè¿‡éšæœºé®è”½å®éªŒï¼Œå¯»æ‰¾æ–‡ä¸­å¯¹â€œåŸè‘—æ„Ÿâ€è´¡çŒ®æœ€å¤§çš„å¥å­... (è¿™å¯èƒ½éœ€è¦åå‡ ç§’)")
                
                # è¿›åº¦æ¡
                lime_progress = st.progress(0)
                
                # 1. å®šä¹‰ LIME é¢„æµ‹å‡½æ•° (æ¡¥æ¥ FastText)
                def predict_proba(texts):
                    results = []
                    # æ¨¡æ‹Ÿè¿›åº¦ï¼šè¿™åªæ˜¯ä¸ªç®€å•çš„ trickï¼Œå› ä¸º predict_proba ä¼šè¢«è°ƒç”¨å‡ ç™¾æ¬¡
                    # å®é™…å¾ˆéš¾ç²¾ç¡®æ§åˆ¶è¿›åº¦æ¡ï¼Œè¿™é‡Œåªèƒ½æ˜¾ç¤ºâ€œæ­£åœ¨è®¡ç®—â€
                    for text in texts:
                        t_tokens = get_style_tokens(text, blocklist)
                        if not t_tokens:
                            results.append([1.0, 0.0])
                            continue
                        vec = get_vec(t_tokens)
                        sim = cosine_similarity([vec], [gold_standard])[0][0]
                        # æ”¾å¤§å·®å¼‚ä»¥ä¾¿ LIME æ›´å¥½æ•æ‰ï¼š(sim^3 å¢åŠ å¯¹æ¯”åº¦)
                        sim_scaled = sim ** 3 
                        results.append([1 - sim_scaled, sim_scaled])
                    return np.array(results)

                # 2. åˆå§‹åŒ–è§£é‡Šå™¨
                explainer = LimeTextExplainer(class_names=['å·®å¼‚', 'åŸè‘—é£'])

                # 3. ä¸­æ–‡åˆ†è¯é€‚é… (å…³é”®æ­¥éª¤)
                # LIME éœ€è¦ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²
                seg_list = jieba.cut(preview_text)
                spaced_text = " ".join(seg_list)

                # 4. ç”Ÿæˆè§£é‡Š (å‡å°‘é‡‡æ ·æ•°ä»¥åŠ å¿«é€Ÿåº¦)
                # num_samples=200 è¶³å¤Ÿæ¼”ç¤ºç”¨
                try:
                    exp = explainer.explain_instance(
                        spaced_text, 
                        predict_proba, 
                        num_features=10, 
                        num_samples=200 
                    )
                    lime_progress.progress(100)
                    
                    # 5. å±•ç¤ºç»“æœ
                    st.write("### ğŸ”¥ æ–‡æœ¬çƒ­åŠ›å›¾")
                    st.caption("é¢œè‰²è¶Šçº¢/æ·±æ©™è‰²ï¼Œä»£è¡¨è¯¥è¯å¥è¶Šå…·æœ‰â€œåŸè‘—ç¥éŸµâ€ï¼›è“è‰²åˆ™ä»£è¡¨ä¸åŸè‘—é£æ ¼ä¸ç¬¦ã€‚")
                    components.html(exp.as_html(), height=600, scrolling=True)
                    
                    # 6. æå–å…³é”®è¯è¡¨
                    st.write("### ğŸ† æ ¸å¿ƒç‰¹å¾è¯")
                    top_features = exp.as_list()
                    # åªè¦æ­£å‘ç‰¹å¾
                    pos_feats = [f for f in top_features if f[1] > 0]
                    if pos_feats:
                        df_feats = pd.DataFrame(pos_feats, columns=["ç‰¹å¾è¯", "åŸè‘—æ„Ÿè´¡çŒ®åº¦"])
                        st.dataframe(df_feats, use_container_width=True)
                    else:
                        st.write("æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„æ­£å‘ç‰¹å¾è¯ã€‚")
                        
                except Exception as e:
                    st.error(f"LIME åˆ†æè¿è¡Œæ—¶å‡ºç°é”™è¯¯: {e}")
                
                status.update(label="å…¨æµç¨‹åˆ†æå·²å®Œæˆï¼", state="complete", expanded=False)

